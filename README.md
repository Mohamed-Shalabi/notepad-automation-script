# Notepad Automation System

A **production-ready Windows desktop automation system** that uses **CLIP-based vision grounding** to detect and interact with the Notepad desktop icon.

## ğŸ¯ Overview

This system demonstrates **semantic vision grounding** - a technique that uses vision-language models to find UI elements based on their meaning, not fixed coordinates or exact pixel matching.

### Key Features

- **Vision-Based Grounding**: Uses OpenAI's CLIP model to find the Notepad icon by understanding "what is a Notepad icon" rather than pixel matching
- **Robust Detection**: Works regardless of icon position, size, theme, or background
- **Full Automation**: Fetches data from API, creates files, saves them, repeats for 10 posts
- **Comprehensive Error Handling**: Retries, timeouts, graceful degradation
- **Detailed Logging**: Complete audit trail of all actions

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Main Orchestrator                         â”‚
â”‚  (Coordinates the complete workflow)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼              â–¼              â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vision    â”‚ â”‚Automation â”‚ â”‚   API    â”‚ â”‚   File     â”‚
â”‚  Grounding  â”‚ â”‚  Module   â”‚ â”‚  Client  â”‚ â”‚  Manager   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Screenshot  â”‚ â”‚  Mouse    â”‚ â”‚  HTTP    â”‚ â”‚ Directory  â”‚
â”‚ CLIP Model  â”‚ â”‚ Keyboard  â”‚ â”‚  Retry   â”‚ â”‚ Path Gen   â”‚
â”‚ Detector    â”‚ â”‚ Window    â”‚ â”‚  Parse   â”‚ â”‚ Verify     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Requirements

- **OS**: Windows 10 or 11
- **Resolution**: 1920Ã—1080 (primary monitor)
- **Python**: 3.10+
- **Desktop**: Notepad shortcut must exist on desktop

## ğŸš€ Quick Start

### 1. Install uv (if not already installed)

```powershell
# Using pip
pip install uv

# Or using PowerShell installer
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 2. Clone and Setup

```powershell
cd notepad-automation

# Create virtual environment and install dependencies
uv sync

# Install the package in development mode
uv pip install -e .
```

### 3. Run the Automation

```powershell
# Using the entry point
uv run notepad-auto

# Or directly
uv run python -m notepad_automation.main
```

## ğŸ“ Project Structure

```
notepad-automation/
â”œâ”€â”€ pyproject.toml                    # Project configuration
â”œâ”€â”€ README.md                         # This file
â”œâ”€â”€ src/
â”‚   â””â”€â”€ notepad_automation/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py                   # Entry point & orchestrator
â”‚       â”œâ”€â”€ config.py                 # Configuration constants
â”‚       â”œâ”€â”€ grounding/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ screenshot.py         # Desktop capture
â”‚       â”‚   â””â”€â”€ detector.py           # CLIP-based detection
â”‚       â”œâ”€â”€ automation/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ mouse.py              # Mouse control
â”‚       â”‚   â”œâ”€â”€ keyboard.py           # Keyboard automation
â”‚       â”‚   â””â”€â”€ window.py             # Window validation
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ client.py             # JSONPlaceholder client
â”‚       â””â”€â”€ files/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ manager.py            # File operations
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ...                           # Test files
â”œâ”€â”€ screenshots/                      # Annotated detection screenshots
â””â”€â”€ logs/                             # Runtime logs
```

## ğŸ”¬ How Vision Grounding Works

### Traditional Template Matching (âŒ Not Used)
```
Screenshot â†’ Find exact pixel pattern â†’ Return coordinates
```
- Requires pre-stored reference images
- Breaks with different themes, sizes, positions
- No semantic understanding

### CLIP-Based Semantic Grounding (âœ… Our Approach)
```
Screenshot â†’ Extract regions â†’ CLIP encode each region
Text prompts â†’ CLIP encode ("a Notepad icon")
Compare embeddings â†’ Rank by similarity â†’ Return best match
```

### Why CLIP?

**CLIP (Contrastive Language-Image Pre-training)** understands the *meaning* of images:

1. **Trained on 400M image-text pairs** from the internet
2. **Maps images and text to the same embedding space**
3. **Can compare arbitrary images to text descriptions**

When we ask "which region looks like a Notepad icon?", CLIP understands:
- What text editors generally look like
- The concept of "notepad" or "document editing"
- Visual patterns associated with such icons

### Detection Pipeline

```python
# 1. Capture fresh screenshot
screenshot = capture_desktop_screenshot()

# 2. Extract candidate regions (sliding window)
candidates = extract_candidates(screenshot, sizes=[48, 64, 80, 96])

# 3. Encode with CLIP vision encoder
image_embeddings = clip.encode_images(candidates)

# 4. Encode text prompts
text_prompts = [
    "a Notepad application icon",
    "a text editor icon",
    "a Windows Notepad shortcut"
]
text_embeddings = clip.encode_text(text_prompts)

# 5. Find best match
similarities = image_embeddings @ text_embeddings.T
best_candidate = candidates[similarities.argmax()]

# 6. Return center coordinates
return best_candidate.center_x, best_candidate.center_y
```

## ğŸ›¡ï¸ Error Handling

| Scenario | Handling |
|----------|----------|
| Icon not found | Retry 3 times with 1s delay |
| Multiple matches | Rank by confidence, select highest |
| API unavailable | Retry with exponential backoff, then abort |
| Notepad won't launch | Retry 2 times, then skip post |
| File save fails | Verify file exists, report error |
| Unexpected popup | Check window title, close if needed |

All failures are:
- **Logged** with full context
- **Non-crashing** (graceful degradation)
- **Reported** in final summary

## ğŸ“Š Output

### Files Created
```
Desktop/
â””â”€â”€ tjm-project/
    â”œâ”€â”€ post_1.txt
    â”œâ”€â”€ post_2.txt
    â”œâ”€â”€ post_3.txt
    â””â”€â”€ ... (10 files total)
```

### File Format
```
Title: sunt aut facere repellat provident occaecati excepturi optio reprehenderit

quia et suscipit
suscipit recusandae consequuntur expedita et cum
reprehenderit molestiae ut ut quas totam
nostrum rerum est autem sunt rem eveniet architecto
```

### Screenshots
```
screenshots/
â”œâ”€â”€ detection_step_0.png        # Detection for first post
â”œâ”€â”€ detection_step_4.png        # Detection for middle post
â””â”€â”€ detection_step_9.png        # Detection for last post
```

### Logs
```
logs/
â””â”€â”€ automation.log              # Complete execution log
```

## ğŸ¤ Interview Discussion Points

### Why CLIP over Template Matching?

1. **Generalization**: Works on any icon appearance without reference images
2. **Semantic Understanding**: Knows what a "text editor" looks like conceptually
3. **Robustness**: Handles theme changes, scaling, partial visibility
4. **Extensibility**: Same approach works for any UI element

### Known Limitations

1. **Speed**: Sliding window + CLIP is slower than direct detection
2. **Very Small Icons**: May not be detected at extreme scales
3. **Ambiguous Icons**: Similar apps (Notepad vs Notepad++) may confuse detector
4. **GPU Recommended**: CPU inference is 5-10x slower

### Potential Improvements

1. **YOLO + CLIP Hybrid**: Use fast object detector for proposals, CLIP for ranking
2. **Fine-tuning**: Train on desktop icon dataset for better accuracy
3. **Caching**: Store embeddings of known desktop regions
4. **Multi-monitor**: Extend to all connected displays

### Performance Characteristics

| Metric | Value |
|--------|-------|
| Screenshot Capture | ~50ms |
| Candidate Extraction | ~200ms |
| CLIP Inference (GPU) | ~100ms per batch |
| CLIP Inference (CPU) | ~1s per batch |
| Total Detection | 3-5s |

## ğŸ“š Dependencies

| Package | Purpose |
|---------|---------|
| `torch` | Deep learning framework |
| `transformers` | CLIP model loading |
| `Pillow` | Image processing |
| `mss` | Fast screenshots |
| `pyautogui` | Mouse/keyboard control |
| `pywin32` | Windows API access |
| `httpx` | HTTP client |
| `opencv-python` | Image operations |

## ğŸ“„ License

MIT License - feel free to use and modify.

## ğŸ™ Acknowledgments

- OpenAI for the CLIP model
- DummyJSON for the test API
- The Python automation community
